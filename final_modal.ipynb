{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "generic-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from os.path import join as opj\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurolab as nl\n",
    "from minepy import MINE\n",
    "import matplotlib.pyplot as plt\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, ShuffleSplit, RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from lshash.lshash import LSHash\n",
    "from minepy import MINE\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from communities.algorithms import louvain_method, girvan_newman\n",
    "from communities.visualization import draw_communities\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from pyitlib.discrete_random_variable import information_mutual_conditional as imc\n",
    "from somlearn import SOM\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from mlxtend.classifier import EnsembleVoteClassifier, StackingClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import RidgeClassifierCV, RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "automotive-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_fusion_feature(fid: int, gene_num=36):\n",
    "    \"\"\"\n",
    "        decompose fusion feature into snps_id and ROI_id\n",
    "        \n",
    "        ROI_id = fid // gene_num\n",
    "        gene_id = fid % gene_num\n",
    "        \n",
    "        More specifically, in our experiment, gene is 36\n",
    "        \n",
    "        return ROI_id: the ROI compose the fusion features `fid`\n",
    "        return snp_id: the SNP compose the fusion features `fid`\n",
    "    \"\"\"\n",
    "    ROI_id = fid // gene_num\n",
    "    gene_id = fid % gene_num\n",
    "    \n",
    "    return ROI_id, gene_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prime-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleVote(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, clfs, oob_features):\n",
    "        self.clfs = clfs.copy()\n",
    "        self.oob_features = oob_features.copy()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicted_probabilitiy = self.predict_proba(X)\n",
    "        return np.argmax(predicted_probabilitiy, axis=1)        \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        predict_proba = []\n",
    "#         for i in tqdm(range(len(self.clfs)), desc='predict_proba'):\n",
    "        for i in range(len(self.clfs)):\n",
    "            predict_proba.append(self.clfs[i].predict_proba(X[:, self.oob_features[i]]))\n",
    "\n",
    "        predicted_probabilitiy = np.sum(predict_proba, axis=0)\n",
    "        \n",
    "        return predicted_probabilitiy / len(self.clfs)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def fit_predict_proba(self, X):\n",
    "        return self.predict_proba(X)\n",
    "    \n",
    "    def predict_w(self, X, w):\n",
    "        predicted_probability = self.predict_proba_w(X, w)\n",
    "        return np.argmax(predicted_probability, axis=1)\n",
    "    \n",
    "    def predict_proba_w(self, X, w):\n",
    "        predict_proba = []\n",
    "#         for i in tqdm(range(len(self.clfs)), desc='predict_proba'):\n",
    "        for i in range(len(self.clfs)):\n",
    "            predict_proba.append(self.clfs[i].predict_proba(X[:, self.oob_features[i]]) * w[i])\n",
    "\n",
    "        predicted_probabilitiy = np.sum(predict_proba, axis=0)\n",
    "        \n",
    "        return predicted_probabilitiy / len(self.clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conditional-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDCOSRM(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimators, n_estimators, max_samples, max_features, bootstrap, bootstrap_features, hash_size, num_hashtables, n_rows, n_columns, num_results):\n",
    "        self.base_estimators = base_estimators\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.bootstrap_features = bootstrap_features\n",
    "        \n",
    "        self.hash_size = hash_size\n",
    "        self.num_hashtables = num_hashtables\n",
    "        \n",
    "        self.n_rows = n_rows\n",
    "        self.n_columns = n_columns\n",
    "        self.num_results = num_results\n",
    "        \n",
    "        self.bagging = BaggingClassifier(base_estimators,\n",
    "                                         n_estimators=n_estimators,\n",
    "                                         max_samples=max_samples,\n",
    "                                        max_features=max_features,\n",
    "                                        bootstrap=bootstrap,\n",
    "                                        bootstrap_features=bootstrap_features, \n",
    "                                        oob_score=True, \n",
    "                                        n_jobs=-1)\n",
    "        \n",
    "        self.som = SOM(n_columns=self.n_columns, n_rows=self.n_rows)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=.3)\n",
    "        self.bagging.fit(train_x, train_y)\n",
    "        \n",
    "        # 集成中所有基学习器及其选择的特征\n",
    "        self.estimators = self.bagging.estimators_\n",
    "        self.features = self.bagging.estimators_features_\n",
    "        \n",
    "        cl_expression = []\n",
    "        self.lsh = LSHash(hash_size=self.hash_size, input_dim=X.shape[1] + valid_x.shape[0], num_hashtables=self.num_hashtables)\n",
    "        \n",
    "        # 预分桶\n",
    "        for i, esti in enumerate(self.estimators):\n",
    "            structure_ = np.zeros((valid_x.shape[1]))\n",
    "            for j in range(self.features[i].shape[0]):\n",
    "                structure_[self.features[i]] += 1\n",
    "            structure_ -= np.mean(structure_)\n",
    "        \n",
    "            functional_ = esti.predict(valid_x[:, self.features[i]])\n",
    "            \n",
    "            # 从结构相似性和功能相似性两个角度来解释estimators之间的相似性\n",
    "            cl_expression_ = np.hstack((structure_, functional_))\n",
    "            cl_expression.append(cl_expression_)\n",
    "            \n",
    "            self.lsh.index(cl_expression_.astype(int), extra_data=str(i))\n",
    "        \n",
    "        self.som_labels = self.som.fit_predict(cl_expression)\n",
    "        cluster_ids = list(set(self.som_labels))\n",
    "        clusters = {i: [] for i in cluster_ids}\n",
    "        \n",
    "        for i in clusters:\n",
    "            estis = np.where(self.som_labels == i)[0].tolist()\n",
    "            clusters[i] = estis\n",
    "            \n",
    "        final_cluster = dict()\n",
    "        for i in cluster_ids:\n",
    "            # 如果某个cluster中的estimators数量小于5， 则将这个cluster中所有estimators分配出去\n",
    "            if len(clusters[i]) < 5:\n",
    "                # 首先统计每个分类器ANN-50 近邻中的最多的类别\n",
    "                c_counter = []\n",
    "                for e in clusters[i]:\n",
    "                    query_res = self.lsh.query(cl_expression[e], num_results=self.num_results)\n",
    "                    # 统计该estimatos ANN 所属的cluster\n",
    "                    neighbor_class = []\n",
    "                    for res in query_res:\n",
    "                        nei = self.som_labels[int(res[0][-1])]\n",
    "                        neighbor_class.append(nei)\n",
    "                    # ANN-50近邻按照major的方式对cluster进行投票\n",
    "                    e_class = Counter(neighbor_class).most_common()[0][0]\n",
    "                    c_counter.append(e_class)\n",
    "                c_id = Counter(c_counter).most_common()[0][0]\n",
    "                if c_id not in final_cluster.keys():\n",
    "                    final_cluster[c_id] = clusters[i]\n",
    "                else:\n",
    "                    final_cluster[c_id].extend(clusters[i])\n",
    "            else:\n",
    "                if i not in final_cluster.keys():\n",
    "                    final_cluster[i] = clusters[i]\n",
    "                else:\n",
    "                    final_cluster[i].extend(clusters[i])\n",
    "        \n",
    "        \n",
    "        keeped_cluster = dict()\n",
    "        for cid, ens in final_cluster.items():\n",
    "            cluster_scale = len(ens)\n",
    "            \n",
    "            # cluster中每个estimators的性能\n",
    "            valid_acc = [metrics.accuracy_score(self.estimators[i].predict(valid_x[:, self.features[i]]), valid_y) for i in ens]\n",
    "#             print(valid_acc)\n",
    "            # cluster中性能最好的estimator\n",
    "            best_acc = np.argmax(valid_acc)\n",
    "        \n",
    "            \n",
    "            # 找到与best estimator最为相似的所有estimators\n",
    "            query_res = self.lsh.query(cl_expression[ens[best_acc]])\n",
    "            sim_ = [int(res[0][-1]) for res in query_res]\n",
    "            # 这些estimators中与best estimators同一个cluster中的前20%被保留\n",
    "            keeped_num = int(cluster_scale * 0.4)\n",
    "            \n",
    "            keeped_ = []\n",
    "            for sim in sim_:\n",
    "                if len(keeped_) < keeped_num:\n",
    "                    if sim in ens:\n",
    "                        keeped_.append(sim)\n",
    "            keeped_cluster[cid] = keeped_\n",
    "        self.keeped_cluster = keeped_cluster\n",
    "\n",
    "        ensembles = []\n",
    "        for k, v in keeped_cluster.items():\n",
    "            if v:\n",
    "                ensembles.append(EnsembleVote([self.estimators[_] for _ in v], [self.features[_] for _ in v]))\n",
    "        self.ensembles = ensembles\n",
    "        \n",
    "        self.votingClassifier = StackingClassifier(ensembles, fit_base_estimators=False, meta_classifier=LogisticRegression(n_jobs=-1))\n",
    "        self.votingClassifier.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "#         predicted_probabilitiy = self.predict_proba(X)\n",
    "#         return np.argmax(predicted_probabilitiy, axis=1)  \n",
    "        return self.votingClassifier.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        predict_proba = []\n",
    "        for i in range(len(self.ensembles)):\n",
    "            predict_proba.append(self.ensembles[i].predict_proba(X))\n",
    "\n",
    "        predicted_probabilitiy = np.sum(predict_proba, axis=0)\n",
    "        \n",
    "        return predicted_probabilitiy / len(self.ensembles)\n",
    "    \n",
    "    def get_coef(self):\n",
    "        w = preprocessing.normalize(self.votingClassifier.meta_clf_.coef_, norm='l2').reshape(-1, )\n",
    "        \n",
    "        coef = np.zeros((data.shape[1], ))\n",
    "\n",
    "        for i, en in enumerate(self.votingClassifier.clfs_):\n",
    "            for fea in en.oob_features:\n",
    "                coef[fea] += w[i]\n",
    "        return coef"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
